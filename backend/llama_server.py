from flask import Flask, request, jsonify
from gpt4all import GPT4All

app = Flask(__name__)

# Chargement du mod√®le
print("ü¶ô Chargement du mod√®le Llama...")
try:
    model = GPT4All("Llama-3.2-1B-Instruct-Q4_0.gguf")
    print("‚úÖ Mod√®le Llama charg√©!")
except Exception as e:
    print(f"‚ùå Erreur chargement mod√®le: {e}")
    model = None

# ‚úÖ C'EST CETTE ROUTE QUI DOIT √äTRE BIEN √âCRITE :
@app.route('/generate', methods=['POST'])  # ‚Üê NE PAS OUBLIER methods=['POST']
def generate():
    if model is None:
        return jsonify({"error": "Mod√®le non charg√©"}), 500
    try:
        data = request.json
        prompt = data.get('prompt', '')
        max_tokens = data.get('max_tokens', 200)

        print(f"üì® Prompt re√ßu : {prompt}")
        response = model.generate(prompt, max_tokens=max_tokens)

        return jsonify({"text": response})

    except Exception as e:
        print(f"‚ùå Erreur g√©n√©ration : {e}")
        return jsonify({"error": str(e)}), 500

# Route de sant√© pour v√©rifier si le mod√®le est bien charg√©
@app.route('/health', methods=['GET'])
def health():
    return jsonify({"status": "healthy", "model_loaded": model is not None})

if __name__ == '__main__':
    print("üöÄ D√©marrage du serveur Llama sur http://0.0.0.0:8000...")
    app.run(port=8000, host='0.0.0.0', debug=True)


@app.route('/api/chat/', methods=['POST'])
def chat():
    data = request.get_json()
    prompt = data.get('message', '')  # ‚úÖ prend tout le prompt
    print("Prompt re√ßu :", prompt)

    try:
        response = model.generate(prompt, max_tokens=200)
        return jsonify({
            "ai_response": response,
            "annonces": [],  # ‚Üê tu peux mettre des vraies annonces ici
            "detected_preferences": {}
        })
    except Exception as e:
        return jsonify({"error": str(e)}), 500
